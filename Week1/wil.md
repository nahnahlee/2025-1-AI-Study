# 📚 기초 인공지능 스터디 1주차

## ✅ 개념 정리

### 🔹 AI, 머신러닝, 딥러닝
- **AI (인공지능)**: 인간의 학습능력, 추론능력, 지각능력을 인공적으로 구현하려는 분야
- **머신러닝**: 컴퓨터가 데이터로부터 패턴을 학습하고, 학습한 내용을 바탕으로 결정을 내리는 기술
  - *모델*: 학습된 패턴을 수학적으로 나타낸 것
- **딥러닝**: 인공신경망을 이용해 모델을 학습시키는 방법
  - *인공신경망*: 인간의 뉴런을 본떠 만든 네트워크로 학습을 하는 모델
  - 예시: FNN, CNN, RNN 등

> **요약**: AI ⊃ 머신러닝 ⊃ 딥러닝

---

### 🔹 지도학습
- 사람이 `x`라는 데이터에 `y`라는 정답(label)을 붙여 학습하는 방법
- 예시: 분류 문제, 번역기 등

### 🔹 비지도학습
- 데이터만 있고 정답이 없는 상태에서 특징을 학습하는 방법
- 예시: 클러스터링(clustering) 알고리즘

---

### 🔹 데이터의 분리
- 전체 데이터를 아래와 같이 나눠서 사용함:
  - **학습 데이터**: 모델을 학습시킬 때 사용
  - **검증 데이터**: 하이퍼파라미터 튜닝에 사용
  - **테스트 데이터**: 최종 평가에 사용

---

### 🔹 기초 모델
- **NNC**, **KNN**, **선형 분류** 등

---

## 🔸 NNC (Nearest Neighbor Classifier)
- 모든 train 데이터를 기억
- 예측할 때 모든 train 데이터와 비교해 **가장 가까운** label 리턴
- 학습 데이터를 넣으면 **정확도 100%**
- 거리 계산 방식:
  1. `|a - b|`
  2. `√[(a - b)^2]`

---

## 🔸 KNN (K-Nearest Neighbor)
- NNC와 비슷하나, **가장 가까운 K개의 label**을 참고
- 다수결로 예측 → `K` 값에 따라 결과 달라짐
- 학습 데이터를 넣어도 **정확도 100% 안 나올 수 있음**
- `K`와 거리 계산 방식은 **검증 데이터로 튜닝**
- 단점:
  - 테스트 시간이 길다
  - 이미지에 작은 변화가 있어도 민감함

---

## 🔸 선형 분류
- 수식: `f(x) = Wx + b`
- 입력: `x` → 예: 32×32×3 이미지 → 3072차원 벡터
- 출력: `score` 벡터 → 예: 10개의 클래스라면 10×1
- 색상 특성에 의존성이 있음
- 평가 위해 **Loss Function**을 사용

---

## 🧪 KNN 실습 정리

- `distance`: KNN 내부 train 데이터와 test 데이터 사이의 거리
- `K=1`: 가장 가까운 한 개의 데이터를 기준으로 분류

---

## 📌 추가 공부

### 🔸 하이퍼파라미터란?
- 최적 모델을 구현하기 위해 사람이 설정하는 변수들
- 예: 학습률, 반복 횟수, 가중치 초기화 방식 등

---

### 🔸 KNN = K-Nearest Neighbor

---

### 🔸 거리 계산 방식: 언제 어떤 걸 써야 할까?

`유클리드 거리 (√[(x - y)^2])` vs `맨해튼 거리 (|x - y|)`

- `유클리드 거리`: 이미지, 센서값, 연속적인 수치들, 대부분 유클리드 거리를 사용
- `맨해튼 거리`: 희소한 벡터 데이터, 노이즈가 많을 때, 차원이 엄청 높을 때